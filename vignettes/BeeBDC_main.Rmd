---
title: "BeeBDC vignette"

output:
   rmarkdown::html_vignette:

vignette: >
  %\VignetteIndexEntry{BeeBDC vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
--- 

```{r libraryChunk, load-packages, include=FALSE}
  # markdown packages
library(rgnparser)
library(magrittr)
library(knitr)
library(rmarkdown)
library(rmdformats)
library(prettydoc)
library(htmltools)
library(pkgdown)

  # Load core packages
library(devtools)
library(BiocManager)
library(purrr)
library(here)
library(renv)
library(bdc)
library(CoordinateCleaner)
library(dplyr)
library(readr)
library(stringr)
library(lubridate)
library(tidyselect)
library(R.utils)
library(tidyr)
library(ggplot2)
library(forcats)
library(emld)
library(rlang)
library(xml2)
library(mgsub)
library(rvest)
library(rnaturalearth)
library(rnaturalearthdata)
library(countrycode)
library(janitor)
library(circlize)
library(paletteer)
library(cowplot)
library(igraph)
library(ggspatial)
library(sf)
library(parallel)
library(terra)
```


```{r secretRootPath, include=FALSE}
  # Use here::i_am to set the root directory
here::i_am("vignettes/BeeBDC_main.Rmd")
  # Set a root path that will work on the local machine and also on GitHub actions
RootPath <- paste0(dirname(getwd()), "/inst/extdata/WebDir")
  # Create the working directory in the RootPath if it doesn't exist already
if (!dir.exists(paste0(RootPath, "/Data_acquisition_workflow"))) {
    dir.create(paste0(RootPath, "/Data_acquisition_workflow"), recursive = TRUE)
}

```

```{r global-options, include=FALSE}
knitr::opts_chunk$set(error = TRUE, 
                      eval = TRUE, 
                      tidy = TRUE, 
                      warning = FALSE,
                      root.dir = normalizePath(paste0(dirname(getwd()), "/inst/extdata/WebDir")))
```


# 0.0 Script preparation 
## 0.1 Working directory 
Choose the path to the root folder in which all other folders can be found.
```{r falseRootPath, eval = FALSE}
RootPath <- paste0("/your/path/here")
```

```{r CreateRootPath, warning=FALSE}
  # Create the working directory in the RootPath if it doesn't exist already
if (!dir.exists(paste0(RootPath, "/Data_acquisition_workflow"))) {
    dir.create(paste0(RootPath, "/Data_acquisition_workflow"), recursive = TRUE)
}
  # Set the working directory
setwd(paste0(RootPath,"/Data_acquisition_workflow"))
```

For the first time that you run BeeBDC, and if you want to use the renv package to manage your 
packages, you can install renv...

            install.packages("renv")

 and then initialise renv the project.
 
            renv::init(project = paste0(RootPath,"/Data_acquisition_workflow")) 
  If you have already initialised a project, you can instead just activate it.
```{r activate}
renv::activate(project = paste0(RootPath,"/Data_acquisition_workflow"))
```

## 0.2 Install packages (if needed) 

You may need to install gdal on your computer. This can be done on a Mac by using Homebrew in the terminal and the command "brew install gdal". 

To start out, you will need to install **BiocManager**, **devtools**, **ComplexHeatmap**, and **rnaturalearthhires** to then install and fully use BeeBDC.
```{r installPackages, message=FALSE, warning=FALSE, results=FALSE}
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("ComplexHeatmap")
```
   
   
```{r rnaturalearthhires, eval=FALSE}
  # Install remotes if needed
if (!require("remotes", quietly = TRUE))
    install.packages("remotes")
  # Download and then load rnaturalearthhires
remotes::install_github("ropensci/rnaturalearthhires")
install.packages("rnaturalearthhires", repos = "https://ropensci.r-universe.dev", type = "source")
library(rnaturalearthhires)
```   
   
   Now install **BeeBDC**.

```{r installBeeBDC, results=TRUE, message=TRUE, eval = TRUE}
devtools::install_github("https://github.com/jbdorey/BeeBDC.git", ref = "main", 
                        force = FALSE,
                        auth_token = "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")
library(BeeBDC)
```

  
  Snapshot the renv environment.
  
```{r snapshot}
renv::snapshot(project = paste0(RootPath,"/Data_acquisition_workflow"),
                 prompt = FALSE)
```
      
  Set up the directories used by BeeBDC. These directories include where the data, figures, reports, etc. will be saved. The RDoc needs to be a path RELATIVE to the RootPath; i.e., the file path from which the two diverge.
  
```{r dirMaker}
BeeBDC::dirMaker(
    RootPath = RootPath,
    RDoc = "vignettes/BeeBDC_main.Rmd") %>%
      # Add paths created by this function to the .GlobalEnv
    list2env(envir = .GlobalEnv)
```
      
## 0.3 Load packages 
      
Load packages.

```{r lapply_library, results=FALSE}
lapply(c("ComplexHeatmap", "magrittr"), 
       library, character.only = TRUE)
```


***
        
# 1.0 Data merge 

<div class="alert alert-info">
  <strong> Attention:</strong> <br>
Although each line of code has been validated, in order to save time knitting the R **markdown** document the next section is display only. If you are not data merging (section 1.0) or preparing the data (section 2.0), feel free to skip to Section 3.0 Initial flags.

</div>

## 1.1 Download ALA data 

Download ALA data and create a new file in the DataPath to put those data into. You should also 
first make an account with ALA in order to download your data — <https://auth.ala.org.au/userdetails/registration/createAccount>
      
      BeeBDC::atlasDownloader(path = DataPath,
               userEmail = "your@email.edu.au",
               atlas = "ALA",
               ALA_taxon = "Apiformes")


##  1.2 Import and merge ALA, SCAN, iDigBio, and GBIF data 
      
Supply the path to where the data is, the save_type is either "csv_files" or "R_file".


      DataImp <- BeeBDC::repoMerge(path = DataPath, 
                      occ_paths = BeeBDC::repoFinder(path = DataPath),
                      save_type = "R_file")



If there is an error in finding a file, run `repoFinder()` by itself to troubleshoot. For example:

      		
      			#BeeBDC::repoFinder(path = DataPath)
      			#OUTPUT:
      			#$ALA_data
      			#[1] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/ALA_galah_path/galah_download_2022-09-15/data.csv"
      
      			#$GBIF_data
      			#[1] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/GBIF_webDL_30Aug2022/0000165-220831081235567/occurrence.txt"
      			#[2] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/GBIF_webDL_30Aug2022/0436695-210914110416597/occurrence.txt"
      			#[3] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/GBIF_webDL_30Aug2022/0436697-210914110416597/occurrence.txt"
      			#[4] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/GBIF_webDL_30Aug2022/0436704-210914110416597/occurrence.txt"
      			#[5] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/GBIF_webDL_30Aug2022/0436732-210914110416597/occurrence.txt"
      			#[6] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/GBIF_webDL_30Aug2022/0436733-210914110416597/occurrence.txt"
      			#[7] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/GBIF_webDL_30Aug2022/0436734-210914110416597/occurrence.txt"
      					
      			#$iDigBio_data
      			#[1] "F:/BeeDataCleaning2022/BeeDataCleaning/BeeDataCleaning/BeeData/iDigBio_webDL_30Aug2022/5aa5abe1-62e0-4d8c-bebf-4ac13bd9e56f/occurrence_raw.csv"
      
      			#$SCAN_data
      			#character(0)
      			#Failing because SCAN_data seems to be missing. Downloaded separatly from the one drive


Load in the most-recent version of these data if needed. This will return a list with: 
 
 1. The occurrence dataset with attributes (.$Data_WebDL)
 2. The appended eml file (.$eml_files)


        DataImp <- BeeBDC::importOccurrences(path = DataPath,
                               fileName = "BeeData_")


## 1.3 Import USGS Data 

The `USGS_formatter()` will find, import, format, and create metadata for the USGS dataset. The pubDate must be in day-month-year format.


      USGS_data <- BeeBDC::USGS_formatter(path = DataPath, pubDate = "19-11-2022")


## 1.4 Formatted Source Importer

Use this importer to find files that have been formatted and need to be added to the larger data file. 

The attributes file must contain "attribute" in its name, and the occurrence file must not.


      Complete_data <- BeeBDC::formattedCombiner(path = DataPath, 
                                    strings = c("USGS_[a-zA-Z_]+[0-9]{4}-[0-9]{2}-[0-9]{2}"), 
                                      # This should be the list-format with eml attached
                                    existingOccurrences = DataImp$Data_WebDL,
                                    existingEMLs = DataImp$eml_files) 


In the column *catalogNumber*, remove ".*specimennumber:" as what comes after should be the USGS number to match for duplicates.


      Complete_data$Data_WebDL <- Complete_data$Data_WebDL %>%
        dplyr::mutate(catalogNumber = stringr::str_replace(catalogNumber,
                                                           pattern = ".*\\| specimennumber:",
                                                           replacement = ""))
      

## 1.5 Save data 

Choose the type of data format you want to use in saving your work in 1.x.


      BeeBDC::dataSaver(path = DataPath,# The main path to look for data in
           save_type = "CSV_file", # "R_file" OR "CSV_file"
           occurrences = Complete_data$Data_WebDL, # The existing datasheet
           eml_files = Complete_data$eml_files, # The existing EML files
           file_prefix = "Fin_") # The prefix for the fileNames
    rm(Complete_data, DataImp)


# 2.0 Data preparation 
The data preparatin section of the script relates mostly to integrating **bee** occurrence datasets and corrections and so may be skipped by many general taxon users.

## 2.1 Standardise datasets

You may either use: 

- (a) the bdc import method (works well with general datasets) ***or*** 
- (b) the jbd import method (works well with above data merge)

### a. bdc import 

The bdc import is **NOT** truly supported here, but provided as an example. Please go to section 2.1b below.
Read in the **bdc** metadata and standardise the dataset to bdc.
      
            bdc_metadata <- readr::read_csv(paste(DataPath, "out_file", "bdc_integration.csv", sep = "/"))
            # ?issue — datasetName is a darwinCore field already!
            # Standardise the dataset to bdc
            db_standardized <- bdc::bdc_standardize_datasets(
              metadata = bdc_metadata,
              format = "csv",
              overwrite = TRUE,
              save_database = TRUE)
            # read in configuration description file of the column header info
            config_description <- readr::read_csv(paste(DataPath, "Output", "bdc_configDesc.csv",
                                                        sep = "/"), 
                                                  show_col_types = FALSE, trim_ws = TRUE)
      
### b. jbd import 

Find the path, read in the file, and add the *database_id* column.

      occPath <- BeeBDC::fileFinder(path = DataPath, fileName = "Fin_BeeData_combined_")


      db_standardized <- readr::read_csv(occPath, 
                                           # Use the basic ColTypeR function to determine types
                                         col_types = BeeBDC::ColTypeR(), trim_ws = TRUE) %>%
                                         dplyr::mutate(database_id = paste("Dorey_data_", 
                                         1:nrow(.), sep = ""),
                                         .before = family)


### c. optional thin


You can thin the dataset for ***TESTING ONLY!*** 

			 check_pf <- check_pf %>%
			   # take every 100th record
			   filter(row_number() %% 100 == 1)

***

## 2.2 Paige dataset

Paige Chesshire's cleaned American dataset — <https://doi.org/10.1111/ecog.06584>
 
### Import data
If you haven't figured it out by now, don't worry about the column name warning — not all columns occur here.


      PaigeNAm <- readr::read_csv(paste(DataPath, "Paige_data", "NorAmer_highQual_only_ALLfamilies.csv",
                                        sep = "/"), col_types = BeeBDC::ColTypeR()) %>%
         # Change the column name from Source to dataSource to match the rest of the data.
        dplyr::rename(dataSource = Source) %>%
         # EXTRACT WAS HERE
          # add a NEW database_id column
        dplyr::mutate(
          database_id = paste0("Paige_data_", 1:nrow(.)),
          .before = scientificName)



<div class="alert alert-info">
  <strong> Attention:</strong> <br>
It is recommended to run the below code on the full bee dataset with more than 16GB RAM. Robert ran this on a laptop with 16GB RAM and an Intel(R) Core(TM) i7-8550U processor (4 cores and 8 threads) — it struggled.

</div>

### Merge Paige's data with downloaded data


      db_standardized <- BeeBDC::PaigeIntegrater(
          db_standardized = db_standardized,
          PaigeNAm = PaigeNAm,
            # This is a list of columns by which to match Paige's data to the most-recent download with. 
            # Each vector will be matched individually
          columnStrings = list(
            c("decimalLatitude", "decimalLongitude", 
              "recordNumber", "recordedBy", "individualCount", "samplingProtocol",
              "associatedTaxa", "sex", "catalogNumber", "institutionCode", "otherCatalogNumbers",
              "recordId", "occurrenceID", "collectionID"),         # Iteration 1
            c("catalogNumber", "institutionCode", "otherCatalogNumbers",
              "recordId", "occurrenceID", "collectionID"), # Iteration 2
            c("decimalLatitude", "decimalLongitude", 
              "recordedBy", "genus", "specificEpithet"),# Iteration 3
            c("id", "decimalLatitude", "decimalLongitude"),# Iteration 4
            c("recordedBy", "genus", "specificEpithet", "locality"), # Iteration 5
            c("recordedBy", "institutionCode", "genus", 
              "specificEpithet","locality"),# Iteration 6
            c("occurrenceID","decimalLatitude", "decimalLongitude"),# Iteration 7
            c("catalogNumber","decimalLatitude", "decimalLongitude"),# Iteration 8
            c("catalogNumber", "locality") # Iteration 9
          ) )


Remove spent data. 

      rm(PaigeNAm)



## 2.3 USGS
 The USGS dataset also partially occurs on GBIF from BISON. However, the occurrence codes are in a silly place... We will correct these here to help identify duplicates later.

        db_standardized <- db_standardized %>%
              # Remove the discoverlife html if it is from USGS
          dplyr::mutate(occurrenceID = dplyr::if_else(
            stringr::str_detect(occurrenceID, "USGS_DRO"),
            stringr::str_remove(occurrenceID, "http://www\\.discoverlife\\.org/mp/20l\\?id="),
            occurrenceID)) %>%
              # Use otherCatalogNumbers when occurrenceID is empty AND when USGS_DRO is detected there
          dplyr::mutate(
            occurrenceID = dplyr::if_else(
              stringr::str_detect(otherCatalogNumbers, "USGS_DRO") & is.na(occurrenceID),
              otherCatalogNumbers, occurrenceID)) %>%
               # Make sure that no eventIDs have snuck into the occurrenceID columns 
               # For USGS_DRO, codes with <6 digits are event ids
          dplyr::mutate(
            occurrenceID = dplyr::if_else(stringr::str_detect(occurrenceID, "USGS_DRO", negate = TRUE),
                 # Keep occurrenceID if it's NOT USGS_DRO
               occurrenceID, 
                 # If it IS USGS_DRO and it has => 6 numbers, keep it, else, NA
              dplyr::if_else(stringr::str_detect(occurrenceID, "USGS_DRO[0-9]{6,10}"),
                             occurrenceID, NA_character_)),
            catalogNumber = dplyr::if_else(stringr::str_detect(catalogNumber, "USGS_DRO", negate = TRUE),
                 # Keep catalogNumber if it's NOT USGS_DRO
              catalogNumber, 
                 # If it IS USGS_DRO and it has => 6 numbers, keep it, else, NA
              dplyr::if_else(stringr::str_detect(catalogNumber, "USGS_DRO[0-9]{6,10}"),
                             catalogNumber, NA_character_)))


## 2.4 Additional datasets
### Import additional and potentially private datasets.
**Note:** Private dataset functions are provided but the data itself is not integrated here until those datasets become freely available.

There will be some warnings were a few rows may not be formatted correctly or where dates fail to parse. This is normal.


###### a. EPEL
Guzman, L. M., Kelly, T. & Elle, E. A data set for pollinator diversity and their interactions with plants in the Pacific Northwest. Ecology, e3927 (2022). <https://doi.org/10.1002/ecy.3927>

    EPEL_Data <- BeeBDC::readr_BeeBDC(dataset = "EPEL",
                                    path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/bee_data_canada.csv",
                          outFile = "jbd_EPEL_data.csv",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
                          
###### b. Allan Smith-Pardo
Data from Allan Smith-Pardo 

    ASP_Data <- BeeBDC::readr_BeeBDC(dataset = "ASP",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/Allan_Smith-Pardo_Dorey_ready2.csv",
                          outFile = "jbd_ASP_data.csv",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
                          
###### c. Minckley
Data from Robert Minckley 

    BMin_Data <- BeeBDC::readr_BeeBDC(dataset = "BMin",
                                    path = paste0(DataPath, "/Additional_Datasets"),
                            inFile = "/InputDatasets/Bob_Minckley_6_1_22_ScanRecent-mod_Dorey.csv",
                            outFile = "jbd_BMin_data.csv",
                            dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
                            
###### d. BMont
Delphia, C. M. Bumble bees of Montana. <https://www.mtent.org/projects/Bumble_Bees/bombus_species.html>. (2022)

    BMont_Data <- BeeBDC::readr_BeeBDC(dataset = "BMont",
                                     path = paste0(DataPath, "/Additional_Datasets"),
                              inFile = "/InputDatasets/Bombus_Montana_dorey.csv",
                              outFile = "jbd_BMont_data.csv",
                              dataLicense = "https://creativecommons.org/licenses/by-sa/4.0/")
                              
###### e. Ecd
Ecdysis. Ecdysis: a portal for live-data arthropod collections, <https://serv.biokic.asu.edu/ecdysis/index.php> (2022).

    Ecd_Data <- BeeBDC::readr_BeeBDC(dataset = "Ecd",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/Ecdysis_occs.csv",
                          outFile = "jbd_Ecd_data.csv",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
                          
###### f. Gai
Gaiarsa, M. P., Kremen, C. & Ponisio, L. C. Pollinator interaction flexibility across scales affects patch colonization and occupancy. *Nature Ecology & Evolution* 5, 787-793 (2021). <https://doi.org/10.1038/s41559-021-01434-y>

    Gai_Data <- BeeBDC::readr_BeeBDC(dataset = "Gai",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/upload_to_scan_Gaiarsa et al_Dorey.csv",
                          outFile = "jbd_Gai_data.csv",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
                          
###### g. CAES

From the Connecticut Agricultural Experiment Station

    CAES_Data <- BeeBDC::readr_BeeBDC(dataset = "CAES",
                                    path = paste0(DataPath, "/Additional_Datasets"),
                            inFile = "/InputDatasets/CT_BEE_DATA_FROM_PBI.xlsx",
                            outFile = "jbd_CT_Data.csv",
                            sheet = "Sheet1",
                            dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")

###### h. GeoL

    GeoL_Data <- BeeBDC::readr_BeeBDC(dataset = "GeoL",
                                    path = paste0(DataPath, "/Additional_Datasets"),
                            inFile = "/InputDatasets/Geolocate and BELS_certain and accurate.xlsx",
                            outFile = "jbd_GeoL_Data.csv",
                            dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")


###### i. EaCO

    EaCO_Data <- BeeBDC::readr_BeeBDC(dataset = "EaCO",
                                    path = paste0(DataPath, "/Additional_Datasets"),
                            inFile = "/InputDatasets/Eastern Colorado bee 2017 sampling.xlsx",
                            outFile = "jbd_EaCo_Data.csv",
                            dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
    

###### j. FSCA

Florida State Collection of Arthropods

    FSCA_Data <- BeeBDC::readr_BeeBDC(dataset = "FSCA",
                                    path = paste0(DataPath, "/Additional_Datasets"),
                            inFile = "InputDatasets/fsca_9_15_22_occurrences.csv",
                            outFile = "jbd_FSCA_Data.csv",
                            dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")

###### k. Texas SMC
Published or unpublished data from Texas literature not in an online database, usually copied into spreadsheet from document format, or otherwise copied from a very differently-formatted spreadsheet. Unpublished or partially published data were obtained with express permission from the lead author.

    SMC_Data <- BeeBDC::readr_BeeBDC(dataset = "SMC",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/TXbeeLitOccs_31Oct22.csv", 
                          outFile = "jbd_SMC_Data.csv",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
                          

###### l. Texas Bal
Data with GPS coordinates from Ballare, K. M., Neff, J. L., Ruppel, R. & Jha, S. Multi-scalar drivers of biodiversity: local management mediates wild bee community response to regional urbanization. *Ecological Applications* 29, e01869 (2019), <https://doi.org/10.1002/eap.1869>. The version on Dryad is missing site GPS coordinates (by accident). Kim is okay with these data being made public as long as her paper is referenced. - Elinor Lichtenberg

    Bal_Data <- BeeBDC::readr_BeeBDC(dataset = "Bal",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/Beedata_ballare.xlsx", 
                          outFile = "jbd_Bal_Data.csv",
                          sheet = "animal_data",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")

###### m. Palouse Lic
Attached: My canola data. I tried to get this in DarwinCore format. These data go with a manuscript currently under review. Here’s the bioRxiv version: Lichtenberg, E. M., Milosavljević, I., Campbell, A. J. & Crowder, D. W. Differential effects of soil conservation practices on arthropods and crop yield. *bioRxiv*, https://doi.org/10.1101/2021.1112.1106.471474 (2022) <https://www.biorxiv.org/content/10.1101/2021.12.06.471474v2>. These are the data I will be putting on SCAN. - Elinor Lichtenberg
 
    Lic_Data <- BeeBDC::readr_BeeBDC(dataset = "Lic",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/Lichtenberg_canola_records.csv", 
                          outFile = "jbd_Lic_Data.csv",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")

###### n. Arm

    Arm_Data <- BeeBDC::readr_BeeBDC(dataset = "Arm",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/Bee database Armando_Final.xlsx",
                          outFile = "jbd_Arm_Data.csv",
                          sheet = "Sheet1",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")

###### o. Dor
From several papers: <https://doi.org/10.3897/jhr.79.57308>, <https://doi.org/10.3897/jhr.81.59365>, and <https://doi.org/10.11646/zootaxa.4674.1.1>.

    Dor_Data <- BeeBDC::readr_BeeBDC(dataset = "Dor",
                                   path = paste0(DataPath, "/Additional_Datasets"),
                          inFile = "/InputDatasets/DoreyData.csv",
                          outFile = "jbd_Dor_Data.csv",
                          dataLicense = "https://creativecommons.org/licenses/by-nc-sa/4.0/")
    

### 2.5 Merge all 
Remove these spent datasets.

      rm(EPEL_Data, ASP_Data, BMin_Data, BMont_Data, Ecd_Data, Gai_Data, CAES_Data, 
      GeoL_Data, EaCO_Data, FSCA_Data, SMC_Data, Bal_Data, Lic_Data, Arm_Data, Dor_Data)
   
Read in and merge all. There are more `readr_BeeBDC()` supported than currently implemented and these represent datasets that will be publicly released in the future. See '?`readr_BeeBDC()`' for details.

      
    db_standardized <- db_standardized %>%
      dplyr::bind_rows(
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_ASP_data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_EPEL_data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_BMin_data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_BMont_data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_Ecd_data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_Gai_data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_CT_Data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_GeoL_Data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_EaCo_Data.csv"), col_types = BeeBDC::ColTypeR()), 
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_SMC_Data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_Bal_Data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_Lic_Data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_Arm_Data.csv"), col_types = BeeBDC::ColTypeR()),
        readr::read_csv(paste0(DataPath, "/Additional_Datasets", 
                               "/jbd_Dor_Data.csv"), col_types = BeeBDC::ColTypeR())) %>% 
        # END bind_rows
      suppressWarnings(classes = "warning") # End suppressWarnings — due to col_types


### 2.6 Match database_id 

If you have prior runs from which you'd like to match *database_id*s with from the current run, you may use the below script to try to match *database_id*s with prior runs. 


Read in a prior run of choice.


      priorRun <- BeeBDC::fileFinder(path = DataPath,
                              file = "01_prefilter_database_9Aug22.csv") %>%
        readr::read_csv(file = ., col_types = BeeBDC::ColTypeR())


This function will attempt to find the *database_id*s from prior runs.

      
      db_standardized <- BeeBDC::idMatchR(
      currentData = db_standardized,
      priorData = priorRun,
        # First matches will be given preference over later ones
      matchBy = tibble::lst(c("gbifID", "dataSource"),
                            c("catalogNumber", "institutionCode", "dataSource", "decimalLatitude",
                              "decimalLongitude"),
                            c("occurrenceID", "dataSource","decimalLatitude","decimalLongitude"),
                            c("recordId", "dataSource","decimalLatitude","decimalLongitude"),
                            c("id", "dataSource","decimalLatitude","decimalLongitude"),
                            # Because INHS was entered as it's own dataset but is now included in the GBIF    download...
                            c("catalogNumber", "institutionCode", "dataSource",
                              "decimalLatitude","decimalLongitude")),
        # You can exclude datasets from prior by matching their prefixs — before first underscore:
      excludeDataset = c("ASP", "BMin", "BMont", "CAES", "EaCO", "Ecd", "EcoS",
                         "Gai", "KP", "EPEL", "CAES", "EaCO", "FSCA", "SMC", "Lic", "Arm"))
    
     # Remove redundant files
    rm(priorRun)

Save the dataset.


      db_standardized %>%
        readr::write_excel_csv(.,
                         paste(OutPath_Intermediate, "00_prefilter_database.csv",
                               sep = "/"))


# 3.0 Initial flags 
Read data back in if needed. OutPath_Intermediate (and a few other directories) should be have been created and saved to the global environment by `dirMaker()`.


    if(!exists("db_standardized")){
      db_standardized <- readr::read_csv(paste(OutPath_Intermediate, "00_prefilter_database.csv",
                                        sep = "/"), col_types = BeeBDC::ColTypeR())}


```{r 3.0}
data("bees3sp", package = "BeeBDC")
data("beesRaw", package = "BeeBDC")
db_standardized <- dplyr::bind_rows(beesRaw, 
                                      # Only keep a subset of columns from bees3sp
                             bees3sp %>% dplyr::select(tidyselect::all_of(colnames(beesRaw)), countryCode))
```




*For more details about the bdc package, please see their  [tutorial.](https://brunobrr.github.io/bdc/articles/prefilter.html)*

 
## 3.1 SciName 
Flag occurrences without *scientificName* provided.

```{r 3.1}
check_pf <- bdc::bdc_scientificName_empty(
  data = db_standardized,
  sci_name = "scientificName")
  # now that this is saved, remove it to save space in memory
rm(db_standardized) 
```


## 3.2 MissCoords 
Flag occurrences with missing *decimalLatitude* and *decimalLongitude*.

```{r 3.2}
check_pf <- bdc::bdc_coordinates_empty(
  data = check_pf,
  lat = "decimalLatitude",
  lon = "decimalLongitude")
```

## 3.3 OutOfRange 
Flag occurrences that are not on Earth (outside of -180 to 180 or -90 to 90 degrees).

```{r 3.3}
check_pf <- bdc::bdc_coordinates_outOfRange(
  data = check_pf,
  lat = "decimalLatitude",
  lon = "decimalLongitude")
```

## 3.4 Source 
Flag occurrences that don't match the *basisOfRecord* types below.

```{r 3.4}
check_pf <- bdc::bdc_basisOfRecords_notStandard(
  data = check_pf,
  basisOfRecord = "basisOfRecord",
  names_to_keep = c(
    # Keep all plus some at the bottom.
    "Event",
    "HUMAN_OBSERVATION",
    "HumanObservation",
    "LIVING_SPECIMEN",
    "LivingSpecimen",
    "MACHINE_OBSERVATION",
    "MachineObservation",
    "MATERIAL_SAMPLE",
    "O",
    "Occurrence",
    "MaterialSample",
    "OBSERVATION",
    "Preserved Specimen",
    "PRESERVED_SPECIMEN",
    "preservedspecimen Specimen",
    "Preservedspecimen",
    "PreservedSpecimen",
    "preservedspecimen",
    "S",
    "Specimen",
    "Taxon",
    "UNKNOWN",
    "",
    NA,
    "NA",
    "LITERATURE", 
    "None", "Pinned Specimen", "Voucher reared", "Emerged specimen"
  ))
```

	   
## 3.5 CountryName 

Try to harmonise country names.  

### a. prepare dataset 

Fix up country names based on common problems above and extract ISO2 codes for occurrences.


```{r 3.5a}
check_pf_noNa <- BeeBDC::countryNameCleanR(
  data = check_pf,
    # Create a Tibble of common issues in country names and their replacements
  commonProblems = dplyr::tibble(problem = c('U.S.A.', 'US','USA','usa','UNITED STATES',
                                              'United States','U.S.A','MX','CA','Bras.','Braz.',
                                              'Brasil','CNMI','USA TERRITORY: PUERTO RICO'),
                                  fix = c('United States of America','United States of America',
                                          'United States of America','United States of America',
                                          'United States of America','United States of America',
                                          'United States of America','Mexico','Canada','Brazil',
                                          'Brazil','Brazil','Northern Mariana Islands','PUERTO.RICO'))
  )
```


### b. run function
Get country name from coordinates using a wrapper around the `jbd_country_from_coordinates()` function. Because our dataset is much larger than those used to design **bdc**, we have made it so that you can analyse data in smaller pieces. Additionally, like some other functions in **BeeBDC**, we have implemented parallel operations (using mc.cores = #cores in stepSize = #rowsPerOperation); see '?`jbd_CfC_chunker()`' for details. 
NOTE: In an actual run you should use scale = "large"

```{r 3.5b, message=FALSE, warning=FALSE}
suppressWarnings(
  countryOutput <- BeeBDC::jbd_CfC_chunker(data = check_pf_noNa,
                                   lat = "decimalLatitude",
                                   lon = "decimalLongitude",
                                   country = "country",
                                    # How many rows to process at a time
                                   stepSize = 1000000,
                                    # Start row
                                   chunkStart = 1,
                                   path = OutPath_Intermediate,
                                   append = FALSE,
                                    # Normally, please use scale = "large"
                                   scale = "medium",
                                   mc.cores = 1),
  classes = "warning")
```

### c. re-merge 

Join these datasets.

```{r 3.5ci}
check_pf <- dplyr::left_join(check_pf, 
                             countryOutput, 
                             by = "database_id",
                             suffix = c("", "CO"))  %>% 
    # Take the new country name if the original is NA
  dplyr::mutate(country = dplyr::if_else(is.na(country),
                                         countryCO,
                                         country)) %>%
    # Remove duplicates if they arose from left_join!
  dplyr::distinct()
```

Save the dataset.

```{r 3.5cii}
check_pf %>%
  readr::write_excel_csv(.,
                   paste(OutPath_Intermediate, "01_prefilter_database.csv",
                         sep = "/"))
```

Read in if needed. 

```{r 3.5ciii}
if(!exists("check_pf")){
check_pf <- readr::read_csv(paste(DataPath, 
             "Output", "Intermediate", "01_prefilter_database.csv", sep = "/"),
             col_types = BeeBDC::ColTypeR())}
```

Remove these interim datasets.

```{r 3.5civ}
rm(check_pf_noNa, countryOutput)
```

## 3.6 StandardCoNames 

Run the function, which standardises country names and adds ISO2 codes, if needed.

```{r 3.6}
  # Standardise country names and add ISO2 codes if needed
check_pf <- bdc::bdc_country_standardized(
  # Remove the countryCode and country_suggested columns to avoid an error with 
    # where two "countryCode" and "country_suggested" columns exist (i.e. if the dataset has been  
    # run before)
  data = check_pf %>% dplyr::select(!tidyselect::any_of(c("countryCode", "country_suggested"))),
  country = "country"
) 
```

## 3.7 TranspCoords 

Flag and correct records when *decimalLatitude* and *decimalLongitude* appear to be transposed. We created this chunked version of  `bdc::bdc_coordinates_transposed()` because it is very RAM-heavy using our large bee dataset. Like many of our other 'jbd_...' functions there are other improvements - e.g., parallel running. 
NOTE: Usually you would use scale = "large", which requires rnaturalearthhires

```{r 3.7, message=FALSE, warning=FALSE}
check_pf <- BeeBDC::jbd_Ctrans_chunker(
  # bdc_coordinates_transposed inputs
  data = check_pf,
  id = "database_id",
  lat = "decimalLatitude",
  lon = "decimalLongitude",
  country = "country",
  countryCode = "countryCode",
  border_buffer = 0.2, # in decimal degrees (~22 km at the equator)
  save_outputs = TRUE,
  sci_names = "scientificName",
  # chunker inputs
  stepSize = 1000000,  # How many rows to process at a time
  chunkStart = 1,  # Start row
  append = FALSE,  # If FALSE it may overwrite existing dataset
    # In a normal run, please use scale = "large"
  scale = "medium",
  path = OutPath_Check,
  mc.cores = 1
) 
```

Get a quick summary of the number of transposed records.

```{r 3.7ii, eval = FALSE}
table(check_pf$coordinates_transposed, useNA = "always")
```

Save the dataset.
```{r 3.7iii}
check_pf %>%
  readr::write_excel_csv(.,
                   paste(OutPath_Intermediate, "01_prefilter_database.csv",
                         sep = "/"))
gc()
```


Read the data in again if needed.

```{r 3.7iv}
if(!exists("check_pf")){
  check_pf <- readr::read_csv(paste(OutPath_Intermediate, "01_prefilter_database.csv",
                                    sep = "/"))}
```


## 3.8 Coord-country 
Collect all country names in the *country_suggested* column. We rebuilt a **bdc** function to flag occurrences where the coordinates are inconsistent with the provided country name.

```{r 3.8}
check_pf <- BeeBDC::jbd_coordCountryInconsistent(
  data = check_pf,
  lon = "decimalLongitude",
  lat = "decimalLatitude",
  mapResolution = 50,
  pointBuffer = 0.01)
```

Save the dataset.
```{r 3.8ii}
check_pf %>%
  readr::write_excel_csv(.,
                   paste(OutPath_Intermediate, "01_prefilter_database.csv",
                         sep = "/"))
```

## 3.9 GeoRefIssue 

This function identifies records whose coordinates can potentially be extracted from locality information, which must be manually checked later.

```{r 3.9, eval = FALSE}
xyFromLocality <- bdc::bdc_coordinates_from_locality(
  data = check_pf,
  locality = "locality",
  lon = "decimalLongitude",
  lat = "decimalLatitude",
  save_outputs = TRUE
) %>%
# Save data 
  readr::write_excel_csv(paste(OutPath_Check, "01_coordinates_from_locality.csv",
                         sep = "/"))
```


Remove spent data.
```{r 3.9ii, eval = FALSE}
rm(xyFromLocality)
```

## 3.10 Flag Absent 
Flag the records marked as "absent".

```{r 3.10}
check_pf <- BeeBDC::flagAbsent(data = check_pf,
                   PresAbs = "occurrenceStatus")
```

## 3.11 flag License 

Flag the records that may not be used according to their license information.

```{r 3.11}
check_pf <- BeeBDC::flagLicense(data = check_pf,
                    strings_to_restrict = "all",
                    # DON'T flag if in the following dataSource(s)
                    excludeDataSource = NULL)
```

## 3.12 GBIF issue 

Flag select issues that are flagged by GBIF.

```{r 3.12}
check_pf <- BeeBDC::GBIFissues(data = check_pf, 
                   issueColumn = "issue", 
                   GBIFflags = c("COORDINATE_INVALID", "ZERO_COORDINATE")) 
```

## 3.13 Flag Reports 
### a. Save flags 

Save the flags so far. This function will make sure that you keep a copy of everything that has been flagged up until now. This will be updated throughout the script and can accessed at the end, so be wary of moving files around manually. However, these data will also still be maintained in the main running file, so this is an optional fail-safe.

```{r 3.13a}
flagFile <- BeeBDC::flagRecorder(
  data = check_pf,
  outPath = paste(OutPath_Report, sep =""),
  fileName = paste0("flagsRecorded_", Sys.Date(),  ".csv"),
    # These are the columns that will be kept along with the flags
  idColumns = c("database_id", "id", "catalogNumber", "occurrenceID", "dataSource"),
    # TRUE if you want to find a file from a previous part of the script to append to
  append = FALSE)

```

Update the *.summary* column

```{r 3.13b}
check_pf <- BeeBDC::summaryFun(
  data = check_pf,
    # Don't filter these columns (or NULL)
  dontFilterThese = NULL,
    # Remove the filtering columns?
  removeFilterColumns = FALSE,
    # Filter to ONLY cleaned data?
  filterClean = FALSE)
```



### c. Reporting 
Use **bdc** to generate reports.

```{r 3.13c, eval = FALSE}
(report <- bdc::bdc_create_report(data = check_pf,
                                  database_id = "database_id",
                                  workflow_step = "prefilter",
                                  save_report = TRUE)
)
```

## 3.14 Save 

Save the intermediate dataset.

```{r 3.14}
check_pf %>%
  readr::write_excel_csv(., paste(OutPath_Intermediate, "01_prefilter_output.csv",
                            sep = "/"))
```



# 4.0 Taxonomy 
*For more information about the corresponding bdc functions used in this section, see their [tutorial](https://brunobrr.github.io/bdc/articles/taxonomy.html). *

Read in the filtered dataset or rename the 3.x dataset for 4.0.

```{r 4.0}
if(!exists("check_pf")){
database <-
  readr::read_csv( paste(OutPath_Intermediate, "01_prefilter_output.csv",
                         sep = "/"), col_types = BeeBDC::ColTypeR())
}else{
    # OR rename and remove
  database <- check_pf
  # Remove spent dataset
  rm(check_pf)}
```

Remove *names_clean* if it already exists (i.e. you have run the following functions before on this dataset before).

```{r 4.0ii}
			database <- database %>%
			  dplyr::select(!tidyselect::any_of("names_clean"))
```

## 4.1 Prep data names 

This step cleans the database's *scientificName* column. 
	
**! MAC**: You might need to install gnparser through terminal — brew
			brew tap gnames/gn
			brew install gnparser
			
			
<div class="alert alert-info">
  <strong> Attention:</strong> <br>
This can be difficult for a Windows install. Ensure you have the most recent version of R, R Studio, and R packages. Also, check package '**rgnparser**' is installed correctly. If you still can not get the below code to work, you may have to download the latest version of 'gnparser' from [here](https://github.com/gnames/gnparser/releases/tag/v1.6.9). You may then need to manually install it and edit your systems environmental variable PATH to locate 'gnparser.exe'. See [here](https://github.com/gnames/gnparser#installation).

</div>			


```{r 4.1, eval = TRUE}
parse_names <-
  bdc::bdc_clean_names(sci_names = database$scientificName, save_outputs = FALSE)
```

Keep only the *.uncer_terms* and *names_clean* columns.

```{r 4.1ii}
parse_names <-
  parse_names %>%
  dplyr::select(.uncer_terms, names_clean)
```

Merge names with the complete dataset.
```{r 4.1iii}
database <- dplyr::bind_cols(database, parse_names)
rm(parse_names)
```

## 4.2 Harmonise taxonomy 
Read in the custom taxonomy file from the BeeBDC package and [Discover Life](http://discoverlife.org) website.

```{r 4.2}
data("beesTaxonomy", package = "BeeBDC")
```


Harmonise the names in the occurrence tibble. This flags the occurrences without a matched name and matches names to their correct name according to [Discover Life](http://discoverlife.org). You can also use multiple cores to achieve this. See '?`harmoniseR()`' for details.

```{r 4.2gc, include=FALSE}
  # Clean up the garbage
gc()
```

```{r 4.2ii}
database <- BeeBDC::harmoniseR(path = DataPath, #The path to a folder that the output can be saved
                       taxonomy = beesTaxonomy, # The formatted taxonomy file
                       data = database,
                       mc.cores = 1)
```


You don't need this file any more...
```{r 4.2iii}
rm(beesTaxonomy)
```



Save the harmonised file.

```{r 4.2iv}
database %>%
  readr::write_excel_csv(.,
                   paste(DataPath, "Output", "Intermediate", "02_taxonomy_database.csv",
                         sep = "/"))
```

## 4.3 Save flags 
Save the flags so far. This will find the most-recent flag file and append your new data to it. You can double-check the data and number of columns if you'd like to be thorough and sure that all of data are intact.

```{r 4.3}
flagFile <- BeeBDC::flagRecorder(
  data = database,
  outPath = paste(OutPath_Report, sep =""),
  fileName = paste0("flagsRecorded_", Sys.Date(),  ".csv"),
  idColumns = c("database_id", "id", "catalogNumber", "occurrenceID", "dataSource"),
  append = TRUE,
  printSummary = TRUE)
```

# 5.0 Space

Read in the latest database.

```{r 5.0}
if(!exists("database")){
database <-
  readr::read_csv(paste(OutPath_Intermediate, "02_taxonomy_database.csv", sep = "/"),
                  col_types = BeeBDC::ColTypeR())}
```

## 5.1 Coordinate precision 
This function identifies records with a coordinate precision below a specified number of decimal places. For example, the precision of a coordinate with 1 decimal place is 11.132 km at the equator, i.e., the scale of a large city. The major difference between the **bdc** and **BeeBDC** functions is that `jbd_coordinates_precision()` will only flag occurrences if BOTH latitude and longitude are rounded (as opposed to only one of these).
Coordinates with one, two, or three decimal places present a precision of ~11.1 km, ~1.1 km, and ~111 m at the equator, respectively.

```{r 5.1}
check_space <-
  BeeBDC::jbd_coordinates_precision(
    data = database,
    lon = "decimalLongitude",
    lat = "decimalLatitude",
    ndec = 2 # number of decimals to be tested
  )
```

Remove the spent dataset.
```{r 5.1ii}
rm(database)
```

Save the resulting file.
```{r 5.1iii}
check_space %>%
  readr::write_excel_csv(.,
                   paste(OutPath_Intermediate, "03_space_inter_database.csv",
                         sep = "/"))
```

